{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Irh8TprLk0pxnM9-jxcAUkrCN9yPwx_s",
      "authorship_tag": "ABX9TyPqzjMQX9l8Z4BaFeF6GgBd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/labib899/text-to-code-seq2seq/blob/main/text_to_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5gOpyra_Ulo"
      },
      "outputs": [],
      "source": [
        "!pip install datasets tqdm nltk seaborn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configs\n",
        "MAX_SRC_LEN = 50\n",
        "MAX_TGT_LEN = 80\n",
        "BATCH_SIZE  = 64\n",
        "\n",
        "EMBED_DIM   = 256\n",
        "HIDDEN_DIM  = 256\n",
        "DROPOUT     = 0.3\n",
        "\n",
        "LEARNING_RATE        = 0.001\n",
        "TEACHER_FORCING_RATIO= 0.5\n",
        "EPOCHS               = 15\n",
        "GRAD_CLIP            = 1\n",
        "SEED                 = 42\n",
        "\n",
        "SPECIAL_TOKENS = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "metadata": {
        "id": "0p1SL7TFTP3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "ds = load_dataset(\"Nan-Do/code-search-net-python\")\n",
        "full_data = ds[\"train\"].shuffle(seed=SEED)\n",
        "full_data = full_data.select(range(10000))  # reduce for Colab\n",
        "\n",
        "train_data = full_data.select(range(8000))\n",
        "val_data   = full_data.select(range(8000, 9000))\n",
        "test_data  = full_data.select(range(9000, 10000))\n",
        "\n",
        "print(len(train_data), len(val_data), len(test_data))"
      ],
      "metadata": {
        "id": "kMOuWccdA8eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization and vocab\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
        "\n",
        "def build_vocab(data, field, max_vocab_size=20000):\n",
        "    counter = Counter()\n",
        "    for item in data:\n",
        "        tokens = tokenize(item[field])\n",
        "        counter.update(tokens)\n",
        "    vocab = SPECIAL_TOKENS + [tok for tok, _ in counter.most_common(max_vocab_size)]\n",
        "    stoi = {tok:i for i, tok in enumerate(vocab)}\n",
        "    itos = {i:tok for tok,i in stoi.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "src_stoi, src_itos = build_vocab(train_data, \"docstring\")\n",
        "tgt_stoi, tgt_itos = build_vocab(train_data, \"code\")\n",
        "\n",
        "print(\"Source vocab:\", len(src_stoi), \"Target vocab:\", len(tgt_stoi))"
      ],
      "metadata": {
        "id": "fDiEr5aqEYwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset and dataloader\n",
        "class CodeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def encode(self, tokens, stoi, max_len):\n",
        "        tokens = tokens[:max_len]\n",
        "        ids = [stoi.get(tok, stoi[\"<unk>\"]) for tok in tokens]\n",
        "        ids = [stoi[\"<sos>\"]] + ids + [stoi[\"<eos>\"]]\n",
        "        ids += [stoi[\"<pad>\"]] * (max_len + 2 - len(ids))\n",
        "        return torch.tensor(ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        src_tokens = tokenize(item[\"docstring\"])\n",
        "        tgt_tokens = tokenize(item[\"code\"])\n",
        "        src_ids = self.encode(src_tokens, src_stoi, MAX_SRC_LEN)\n",
        "        tgt_ids = self.encode(tgt_tokens, tgt_stoi, MAX_TGT_LEN)\n",
        "        return src_ids, tgt_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(CodeDataset(train_data), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = torch.utils.data.DataLoader(CodeDataset(val_data), batch_size=BATCH_SIZE)\n",
        "test_loader  = torch.utils.data.DataLoader(CodeDataset(test_data), batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "46GnEFJWEuPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vanillaRNN\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        _, hidden = self.rnn(embedded)\n",
        "        return hidden\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_token, hidden):\n",
        "        embedded = self.embedding(input_token)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=TEACHER_FORCING_RATIO):\n",
        "        batch_size = tgt.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(device)\n",
        "        hidden = self.encoder(src)\n",
        "        input_token = tgt[:,0].unsqueeze(1)\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden = self.decoder(input_token, hidden)\n",
        "            outputs[:, t] = output.squeeze(1)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(2)\n",
        "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "qAVAE8BOI6vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        _, (hidden, _) = self.rnn(embedded)\n",
        "        return hidden\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_token, hidden):\n",
        "        embedded = self.embedding(input_token)\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, torch.zeros_like(hidden)))\n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=TEACHER_FORCING_RATIO):\n",
        "        batch_size = tgt.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(device)\n",
        "        hidden = self.encoder(src)  # [1, batch, hidden_dim]\n",
        "\n",
        "        input_token = tgt[:,0].unsqueeze(1)  # <sos>\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden = self.decoder(input_token, hidden)\n",
        "            outputs[:, t] = output.squeeze(1)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(2)\n",
        "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "X5IW_vUFJIlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM + attention\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(enc_hidden_dim*2 + dec_hidden_dim, dec_hidden_dim)\n",
        "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        hidden = hidden.permute(1,0,2).repeat(1, src_len, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden = torch.tanh(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)).unsqueeze(0)\n",
        "        return outputs, hidden\n",
        "\n",
        "class DecoderLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, enc_hidden_dim, dec_hidden_dim, dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim + enc_hidden_dim*2, dec_hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(enc_hidden_dim*2 + dec_hidden_dim + embed_dim, vocab_size)\n",
        "        self.attention = BahdanauAttention(enc_hidden_dim, dec_hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_token, hidden, encoder_outputs):\n",
        "        embedded = self.dropout(self.embedding(input_token))\n",
        "        a = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, torch.zeros_like(hidden)))\n",
        "        output_fc = self.fc(torch.cat((output, weighted, embedded), dim=2))\n",
        "        return output_fc, hidden, a.squeeze(1)\n",
        "\n",
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=TEACHER_FORCING_RATIO):\n",
        "        batch_size = tgt.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input_token = tgt[:,0].unsqueeze(1)\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden, _ = self.decoder(input_token, hidden, encoder_outputs)\n",
        "            outputs[:, t] = output.squeeze(1)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(2)\n",
        "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ztUeREJPJOA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and eval functions\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "        output_dim = output.shape[-1]\n",
        "        loss = criterion(output[:,1:].reshape(-1, output_dim), tgt[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
        "            output_dim = output.shape[-1]\n",
        "            loss = criterion(output[:,1:].reshape(-1, output_dim), tgt[:,1:].reshape(-1))\n",
        "            val_loss += loss.item()\n",
        "    return val_loss / len(loader)\n",
        "\n",
        "def decode_tokens(output_tensor):\n",
        "    return [output_tensor.argmax(-1).cpu().item()]\n",
        "\n",
        "def evaluate_metrics(model, loader, tgt_itos):\n",
        "    model.eval()\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "    references, hypotheses = [], []\n",
        "    exact_matches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
        "            pred_tokens = output.argmax(-1).cpu().numpy()\n",
        "            tgt_tokens = tgt.cpu().numpy()\n",
        "\n",
        "            # Token-level accuracy\n",
        "            mask = tgt_tokens[:,1:] != tgt_stoi[\"<pad>\"]\n",
        "            correct_tokens += ((pred_tokens[:,1:] == tgt_tokens[:,1:]) * mask).sum()\n",
        "            total_tokens += mask.sum()\n",
        "\n",
        "            # BLEU\n",
        "            for t, p in zip(tgt_tokens, pred_tokens):\n",
        "                references.append([ [tgt_itos[i] for i in t if i not in [tgt_stoi[\"<pad>\"], tgt_stoi[\"<sos>\"], tgt_stoi[\"<eos>\"]]] ])\n",
        "                hypotheses.append([ tgt_itos[i] for i in p if i not in [tgt_stoi[\"<pad>\"], tgt_stoi[\"<sos>\"], tgt_stoi[\"<eos>\"]]])\n",
        "\n",
        "            # Exact Match\n",
        "            for t, p in zip(tgt_tokens, pred_tokens):\n",
        "                t_clean = [tgt_itos[i] for i in t if i not in [tgt_stoi[\"<pad>\"], tgt_stoi[\"<sos>\"], tgt_stoi[\"<eos>\"]]]\n",
        "                p_clean = [tgt_itos[i] for i in p if i not in [tgt_stoi[\"<pad>\"], tgt_stoi[\"<sos>\"], tgt_stoi[\"<eos>\"]]]\n",
        "                if t_clean == p_clean:\n",
        "                    exact_matches += 1\n",
        "\n",
        "    token_acc = correct_tokens / total_tokens\n",
        "    bleu = corpus_bleu(references, hypotheses)\n",
        "    exact_match = exact_matches / len(loader.dataset)\n",
        "    return token_acc, bleu, exact_match"
      ],
      "metadata": {
        "id": "dLE6ln7nJjsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train models\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "model_configs = {\n",
        "    \"VanillaRNN\": (EncoderRNN(len(src_stoi), EMBED_DIM, HIDDEN_DIM),\n",
        "                   DecoderRNN(len(tgt_stoi), EMBED_DIM, HIDDEN_DIM)),\n",
        "    \"LSTM\": (EncoderLSTM(len(src_stoi), EMBED_DIM, HIDDEN_DIM),\n",
        "             DecoderLSTM(len(tgt_stoi), EMBED_DIM, HIDDEN_DIM)),\n",
        "    \"Attention\": (EncoderBiLSTM(len(src_stoi), EMBED_DIM, HIDDEN_DIM),\n",
        "                  DecoderLSTMWithAttention(len(tgt_stoi), EMBED_DIM, HIDDEN_DIM, HIDDEN_DIM*2, DROPOUT))\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, (encoder, decoder) in model_configs.items():\n",
        "    print(f\"\\n=== Training {model_name} ===\")\n",
        "    if model_name == \"VanillaRNN\":\n",
        "      model = Seq2Seq(encoder, decoder).to(device)\n",
        "    elif model_name == \"LSTM\":\n",
        "        model = Seq2SeqLSTM(encoder, decoder).to(device)\n",
        "    else:  # Attention\n",
        "        model = Seq2SeqWithAttention(encoder, decoder).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_stoi[\"<pad>\"])\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        # ---- Training ----\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "        for src, tgt in loop:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt)\n",
        "            output_dim = output.shape[-1]\n",
        "            loss = criterion(output[:,1:].reshape(-1, output_dim), tgt[:,1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            loop.set_postfix(train_loss=train_loss/len(loop))\n",
        "\n",
        "        # ---- Validation ----\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for src, tgt in val_loader:\n",
        "                src, tgt = src.to(device), tgt.to(device)\n",
        "                output = model(src, tgt, teacher_forcing_ratio=0)\n",
        "                output_dim = output.shape[-1]\n",
        "                loss = criterion(output[:,1:].reshape(-1, output_dim), tgt[:,1:].reshape(-1))\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), f\"{model_name}_best.pth\")\n",
        "            print(\"Saved checkpoint!\")\n",
        "\n",
        "    # ---- Evaluation ----\n",
        "    model.load_state_dict(torch.load(f\"{model_name}_best.pth\"))\n",
        "    token_acc, bleu, exact = evaluate_metrics(model, test_loader, tgt_itos)\n",
        "    results[model_name] = {\"Token Accuracy\": token_acc, \"BLEU\": bleu, \"Exact Match\": exact}"
      ],
      "metadata": {
        "id": "fGJq0YSgP5fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Final Comparison ===\")\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}: Token Acc: {metrics['Token Accuracy']*100:.2f}%, BLEU: {metrics['BLEU']:.2f}, Exact Match: {metrics['Exact Match']*100:.2f}%\")"
      ],
      "metadata": {
        "id": "4nX1lW_bd2LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train vanillaRNN\n",
        "encoder = EncoderRNN(len(src_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "decoder = DecoderRNN(len(tgt_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_stoi[\"<pad>\"])\n",
        "best_val_loss = float(\"inf\")\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    loop = tqdm(train_loader, desc=f\"VanillaRNN Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "    for src, tgt in loop:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "        output_dim = output.shape[-1]\n",
        "        loss = criterion(output[:,1:].reshape(-1, output_dim), tgt[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        loop.set_postfix(train_loss=epoch_loss/len(loop))\n",
        "\n",
        "    train_losses.append(epoch_loss/len(train_loader))\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Val Loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"VanillaRNN_best.pth\")\n",
        "        torch.save({\"train\": train_losses, \"val\": val_losses}, \"VanillaRNN_losses.pth\")\n",
        "        print(\"Saved Vanilla RNN checkpoint!\")"
      ],
      "metadata": {
        "id": "RIml5xOI7b-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train LSTM\n",
        "encoder = EncoderLSTM(len(src_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "decoder = DecoderLSTM(len(tgt_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "model = Seq2SeqLSTM(encoder, decoder).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_stoi[\"<pad>\"])\n",
        "best_val_loss = float(\"inf\")\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    loop = tqdm(train_loader, desc=f\"LSTM Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "    for src, tgt in loop:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "        output_dim = output.shape[-1]\n",
        "        loss = criterion(output[:,1:].reshape(-1, output_dim), tgt[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        loop.set_postfix(train_loss=epoch_loss/len(loop))\n",
        "\n",
        "    train_losses.append(epoch_loss/len(train_loader))\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Val Loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"LSTM_best.pth\")\n",
        "        torch.save({\"train\": train_losses, \"val\": val_losses}, \"LSTM_losses.pth\")\n",
        "        print(\"Saved LSTM checkpoint!\")"
      ],
      "metadata": {
        "id": "shhcAXQx7saq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train LSTM + attention\n",
        "encoder = EncoderBiLSTM(len(src_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "decoder = DecoderLSTMWithAttention(len(tgt_stoi), EMBED_DIM, HIDDEN_DIM, HIDDEN_DIM*2, DROPOUT)\n",
        "model = Seq2SeqWithAttention(encoder, decoder).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_stoi[\"<pad>\"])\n",
        "best_val_loss = float(\"inf\")\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    loop = tqdm(train_loader, desc=f\"Attention Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "    for src, tgt in loop:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "        output_dim = output.shape[-1]\n",
        "        loss = criterion(output[:,1:].reshape(-1, output_dim), tgt[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        loop.set_postfix(train_loss=epoch_loss/len(loop))\n",
        "\n",
        "    train_losses.append(epoch_loss/len(train_loader))\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Val Loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"Attention_best.pth\")\n",
        "        torch.save({\"train\": train_losses, \"val\": val_losses}, \"Attention_losses.pth\")\n",
        "        print(\"Saved Attention checkpoint!\")"
      ],
      "metadata": {
        "id": "B1L7BmYD8uwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics\n",
        "results = {}\n",
        "\n",
        "for model_name, model in zip([\"VanillaRNN\",\"LSTM\",\"Attention\"], [vanilla_model,lstm_model,attn_model]):\n",
        "    token_acc, bleu, exact = evaluate_metrics(model, test_loader, tgt_itos)\n",
        "    results[model_name] = {\"Token Accuracy\": token_acc,\n",
        "                           \"BLEU\": bleu,\n",
        "                           \"Exact Match\": exact}\n",
        "\n",
        "# Display\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}: Token Acc: {metrics['Token Accuracy']*100:.2f}%, BLEU: {metrics['BLEU']:.2f}, Exact Match: {metrics['Exact Match']*100:.2f}%\")"
      ],
      "metadata": {
        "id": "CPCf7TcF8_I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load models\n",
        "def load_model(model_name):\n",
        "    if model_name == \"VanillaRNN\":\n",
        "        encoder = EncoderRNN(len(src_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "        decoder = DecoderRNN(len(tgt_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "        model = Seq2Seq(encoder, decoder).to(device)\n",
        "    elif model_name == \"LSTM\":\n",
        "        encoder = EncoderLSTM(len(src_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "        decoder = DecoderLSTM(len(tgt_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "        model = Seq2SeqLSTM(encoder, decoder).to(device)\n",
        "    else:  # Attention\n",
        "        encoder = EncoderBiLSTM(len(src_stoi), EMBED_DIM, HIDDEN_DIM)\n",
        "        decoder = DecoderLSTMWithAttention(len(tgt_stoi), EMBED_DIM, HIDDEN_DIM, HIDDEN_DIM*2, DROPOUT)\n",
        "        model = Seq2SeqWithAttention(encoder, decoder).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(f\"{model_name}_best.pth\"))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "vanilla_model = load_model(\"VanillaRNN\")\n",
        "lstm_model    = load_model(\"LSTM\")\n",
        "attn_model    = load_model(\"Attention\")"
      ],
      "metadata": {
        "id": "_404hWrKCHYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for model_name in [\"VanillaRNN\", \"LSTM\", \"Attention\"]:\n",
        "    losses = torch.load(f\"{model_name}_losses.pth\")\n",
        "    plt.plot(losses[\"train\"], label=f\"{model_name} Train\")\n",
        "    plt.plot(losses[\"val\"], label=f\"{model_name} Val\")\n",
        "\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training & Validation Loss Curves\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lLB-g-JX9bKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze errors\n",
        "def analyze_errors(model, loader):\n",
        "    errors = {\"syntax\":0, \"indent\":0, \"operators\":0}\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
        "            pred_tokens = output.argmax(-1).cpu().numpy()\n",
        "            tgt_tokens = tgt.cpu().numpy()\n",
        "            for t, p in zip(tgt_tokens, pred_tokens):\n",
        "                pred_code = \" \".join([tgt_itos[i] for i in p if i not in [tgt_stoi[\"<pad>\"], tgt_stoi[\"<sos>\"], tgt_stoi[\"<eos>\"]]])\n",
        "                total += 1\n",
        "                # Syntax check\n",
        "                try:\n",
        "                    compile(pred_code, \"<string>\", \"exec\")\n",
        "                except SyntaxError:\n",
        "                    errors[\"syntax\"] += 1\n",
        "                # Rough indentation check\n",
        "                if pred_code.count(\"    \") % 4 != 0:\n",
        "                    errors[\"indent\"] += 1\n",
        "                # Operator check\n",
        "                for op in [\">\",\"<\",\"+\",\"-\",\"*\",\"/\"]:\n",
        "                    if op in pred_code:\n",
        "                        break\n",
        "                else:\n",
        "                    errors[\"operators\"] += 1\n",
        "    for k in errors:\n",
        "        errors[k] = errors[k]/total * 100\n",
        "    return errors\n",
        "\n",
        "vanilla_errors = analyze_errors(vanilla_model, test_loader)\n",
        "lstm_errors    = analyze_errors(lstm_model, test_loader)\n",
        "attn_errors    = analyze_errors(attn_model, test_loader)\n",
        "\n",
        "print(\"Syntax/Indent/Operator errors (%)\")\n",
        "print(\"VanillaRNN:\", vanilla_errors)\n",
        "print(\"LSTM:\", lstm_errors)\n",
        "print(\"Attention:\", attn_errors)"
      ],
      "metadata": {
        "id": "YUnX0k6i9gDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performance vs docstring\n",
        "import pandas as pd\n",
        "\n",
        "def performance_vs_length(model, loader):\n",
        "    lengths, exact_matches = [], []\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
        "            pred_tokens = output.argmax(-1).cpu().numpy()\n",
        "            tgt_tokens = tgt.cpu().numpy()\n",
        "            for s, t, p in zip(src, tgt_tokens, pred_tokens):\n",
        "                doc_len = (s != src_stoi[\"<pad>\"]).sum().item()\n",
        "                t_clean = [tgt_itos[i] for i in t if i not in [tgt_stoi[\"<pad>\"], tgt_stoi[\"<sos>\"], tgt_stoi[\"<eos>\"]]]\n",
        "                p_clean = [tgt_itos[i] for i in p if i not in [tgt_stoi[\"<pad>\"], tgt_stoi[\"<sos>\"], tgt_stoi[\"<eos>\"]]]\n",
        "                exact_matches.append(int(t_clean == p_clean))\n",
        "                lengths.append(doc_len)\n",
        "    df = pd.DataFrame({\"doc_len\": lengths, \"exact\": exact_matches})\n",
        "    return df.groupby(\"doc_len\")[\"exact\"].mean()\n",
        "\n",
        "vanilla_perf = performance_vs_length(vanilla_model, test_loader)\n",
        "lstm_perf    = performance_vs_length(lstm_model, test_loader)\n",
        "attn_perf    = performance_vs_length(attn_model, test_loader)\n",
        "\n",
        "plt.plot(vanilla_perf, label=\"VanillaRNN\")\n",
        "plt.plot(lstm_perf, label=\"LSTM\")\n",
        "plt.plot(attn_perf, label=\"Attention\")\n",
        "plt.xlabel(\"Docstring Length\")\n",
        "plt.ylabel(\"Exact Match\")\n",
        "plt.title(\"Performance vs Docstring Length\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zBg4Q9as9o7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attention analysis\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def visualize_attention(model, src_ids, tgt_ids):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_ids.unsqueeze(0).to(device))\n",
        "        input_token = tgt_ids[0].unsqueeze(0).unsqueeze(0).to(device)\n",
        "        attentions = []\n",
        "        for t in range(1, len(tgt_ids)):\n",
        "            output, hidden, attn = model.decoder(input_token, hidden, encoder_outputs)\n",
        "            attentions.append(attn.cpu().numpy())\n",
        "            top1 = output.argmax(2)\n",
        "            input_token = top1\n",
        "        return np.stack(attentions)\n",
        "\n",
        "# Pick 3 test examples\n",
        "for i in [0,1,2]:\n",
        "    src_tokens = tokenize(test_data[i][\"docstring\"])[:MAX_SRC_LEN]\n",
        "    tgt_tokens = tokenize(test_data[i][\"code\"])[:MAX_TGT_LEN]\n",
        "    src_ids = torch.tensor([src_stoi.get(tok, src_stoi[\"<unk>\"]) for tok in src_tokens])\n",
        "    tgt_ids = torch.tensor([tgt_stoi.get(tok, tgt_stoi[\"<unk>\"]) for tok in tgt_tokens])\n",
        "\n",
        "    attention_matrix = visualize_attention(attn_model, src_ids, tgt_ids)\n",
        "    sns.heatmap(attention_matrix, xticklabels=src_tokens, yticklabels=tgt_tokens)\n",
        "    plt.xlabel(\"Docstring Tokens\")\n",
        "    plt.ylabel(\"Generated Code Tokens\")\n",
        "    plt.title(f\"Attention Heatmap Example {i+1}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1DjYZV1b90-u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}